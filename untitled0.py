# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tJ2bf1eqyaur_5fOL7xXGB17CPssdNvu
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load the 3 Excel files
stores = pd.read_excel("stores data-set.xlsx")
sales = pd.read_excel("sales data-set.xlsx")
features = pd.read_excel("Features data set.xlsx")

# Show shape (rows, columns)
print("Stores table shape:", stores.shape)
print("Sales table shape:", sales.shape)
print("Features table shape:", features.shape)

import os
os.listdir()

import pandas as pd

stores = pd.read_csv("stores data-set.csv")
sales = pd.read_csv("sales data-set.csv")
features = pd.read_csv("Features data set.csv")

print("Stores shape:", stores.shape)
print("Sales shape:", sales.shape)
print("Features shape:", features.shape)

stores.head()

sales.head()

features.head()

print("Missing values in STORES table:\n", stores.isna().sum())
print("\nMissing values in SALES table:\n", sales.isna().sum())
print("\nMissing values in FEATURES table:\n", features.isna().sum())

print("Stores % missing:\n", stores.isna().mean() * 100)
print("\nSales % missing:\n", sales.isna().mean() * 100)
print("\nFeatures % missing:\n", features.isna().mean() * 100)

print("Stores columns with missing values:\n", stores.isna().mean()[stores.isna().mean() > 0])
print("\nSales columns with missing values:\n", sales.isna().mean()[sales.isna().mean() > 0])
print("\nFeatures columns with missing values:\n", features.isna().mean()[features.isna().mean() > 0])

print("Stores table STORE IDs:", stores['Store'].nunique())
print("Sales table STORE IDs:", sales['Store'].nunique())
print("Features table STORE IDs:", features['Store'].nunique())

missing_stores = set(sales['Store'].unique()) - set(features['Store'].unique())
missing_stores

print("Unique dates in SALES:", sales['Date'].nunique())
print("Unique dates in FEATURES:", features['Date'].nunique())

missing_dates = set(sales['Date'].unique()) - set(features['Date'].unique())
missing_dates

sales['Dept'].nunique()

duplicates = sales.duplicated(subset=['Store', 'Dept', 'Date']).sum()
duplicates

merged = sales.merge(features, on=['Store','Date'], how='left')
merged.isna().sum()

sales[sales['Weekly_Sales'] < 0]

features[(features['Temperature'] < -50) | (features['Temperature'] > 60)]

features['CPI'].describe()

features[features['CPI'] < 50]

features[features['CPI'] > 300]

features[(features['Unemployment'] < 0) | (features['Unemployment'] > 20)]

features[(features['Fuel_Price'] < 1) | (features['Fuel_Price'] > 10)]

sales.duplicated().sum()

features.dtypes

sales['Date'] = pd.to_datetime(sales['Date'])
features['Date'] = pd.to_datetime(features['Date'])

sales['Date'] = pd.to_datetime(sales['Date'], format='mixed', dayfirst=True, errors='coerce')
features['Date'] = pd.to_datetime(features['Date'], format='mixed', dayfirst=True, errors='coerce')

sales['Date'].head()
features['Date'].head()

sales['Date'].isna().sum(), features['Date'].isna().sum()

# ============================================================
# STEP 0 — Install ReportLab (for PDF generation)
# ============================================================
!pip install reportlab

# ============================================================
# STEP 1 — UPLOAD FILES
# ============================================================
from google.colab import files
uploaded = files.upload()

# ============================================================
# STEP 2 — IMPORT LIBRARIES
# ============================================================
import pandas as pd
import numpy as np
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import letter

# ============================================================
# STEP 3 — LOAD THE 3 CSV FILES
# ============================================================

# CHANGE THESE NAMES IF YOUR UPLOAD NAMES ARE DIFFERENT (check uploaded.keys())
stores = pd.read_csv("stores data-set.csv")
sales = pd.read_csv("sales data-set.csv")
features = pd.read_csv("Features data set.csv")

# Convert dates safely (mixed formats)
sales['Date'] = pd.to_datetime(sales['Date'], format='mixed', dayfirst=True, errors='coerce')
features['Date'] = pd.to_datetime(features['Date'], format='mixed', dayfirst=True, errors='coerce')

# ============================================================
# STEP 4 — COMPLETENESS ANALYSIS
# ============================================================

def missing_summary(df):
    return pd.DataFrame({
        "Missing Count": df.isna().sum(),
        "Missing %": round(df.isna().mean() * 100, 2)
    })

stores_missing = missing_summary(stores)
sales_missing = missing_summary(sales)
features_missing = missing_summary(features)

# ============================================================
# STEP 5 — CONSISTENCY ANALYSIS
# ============================================================

# 1. Compare store IDs
stores_unique = stores['Store'].nunique()
sales_unique = sales['Store'].nunique()
features_unique = features['Store'].nunique()

# Stores missing in features
missing_store_matches = set(sales['Store'].unique()) - set(features['Store'].unique())

# 2. Compare dates
sales_dates = sales['Date'].nunique()
feature_dates = features['Date'].nunique()
missing_dates = set(sales['Date'].unique()) - set(features['Date'].unique())

# 3. Duplicate primary keys
duplicates = sales.duplicated(subset=['Store', 'Dept', 'Date']).sum()

# 4. Join consistency check
merged = sales.merge(features, on=['Store', 'Date'], how='left')
join_missing = merged.isna().sum()

# ============================================================
# STEP 6 — ACCURACY / VALIDITY ANALYSIS
# ============================================================

# Negative Weekly Sales
negative_sales = sales[sales['Weekly_Sales'] < 0]

# Temperature unrealistic
bad_temp = features[(features['Temperature'] < -50) | (features['Temperature'] > 60)]

# CPI unrealistic
bad_cpi_low = features[features['CPI'] < 50]
bad_cpi_high = features[features['CPI'] > 300]

# Unemployment unrealistic
bad_unemp = features[(features['Unemployment'] < 0) | (features['Unemployment'] > 20)]

# Data type issues
types_sales = str(sales.dtypes)
types_features = str(features.dtypes)

# ============================================================
# STEP 7 — GENERATE PDF REPORT
# ============================================================

styles = getSampleStyleSheet()
report = SimpleDocTemplate("DataQuality_Report.pdf", pagesize=letter)
flow = []

def add_heading(text):
    flow.append(Paragraph(f"<b>{text}</b>", styles["Heading2"]))
    flow.append(Spacer(1, 12))

def add_text(text):
    flow.append(Paragraph(text, styles["Normal"]))
    flow.append(Spacer(1, 12))

# Title
add_heading("Data Quality Analysis Report – (Completeness, Consistency, Accuracy/Validity)")

# ===================== COMPLETENESS =====================
add_heading("1. Completeness Analysis")

add_text("Stores Missing Values:<br/>" + stores_missing.to_html())
add_text("Sales Missing Values:<br/>" + sales_missing.to_html())
add_text("Features Missing Values:<br/>" + features_missing.to_html())

# ===================== CONSISTENCY =====================
add_heading("2. Consistency Analysis")

add_text(f"Unique Stores in Stores Table: {stores_unique}")
add_text(f"Unique Stores in Sales Table: {sales_unique}")
add_text(f"Unique Stores in Features Table: {features_unique}")
add_text(f"Stores in Sales but not in Features: {list(missing_store_matches)}")

add_text(f"Unique Dates in Sales: {sales_dates}")
add_text(f"Unique Dates in Features: {feature_dates}")
add_text(f"Dates in Sales not in Features: {list(missing_dates)}")

add_text(f"Duplicate (Store, Dept, Date) Keys in Sales: {duplicates}")
add_text("Missing after Sales–Features join:<br/>" + join_missing.to_string())

# ===================== ACCURACY / VALIDITY =====================
add_heading("3. Accuracy / Validity Analysis")

add_text(f"Negative Weekly Sales Records: {len(negative_sales)}")
add_text(f"Unrealistic Temperature Records: {len(bad_temp)}")
add_text(f"Unrealistic Low CPI Records: {len(bad_cpi_low)}")
add_text(f"Unrealistic High CPI Records: {len(bad_cpi_high)}")
add_text(f"Invalid Unemployment Records: {len(bad_unemp)}")
add_text(f"Sales Data Types:<br/>{types_sales}")
add_text(f"Features Data Types:<br/>{types_features}")

# Build PDF
report.build(flow)

# ============================================================
# STEP 8 — DOWNLOAD PDF
# ============================================================
files.download("DataQuality_Report.pdf")